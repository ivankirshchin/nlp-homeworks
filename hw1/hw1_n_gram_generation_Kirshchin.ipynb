{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубинное обучение для текстовых данных, ФКН ВШЭ\n",
    "\n",
    "## Домашнее задание 1: Text Suggestion\n",
    "\n",
    "__Мягкий дедлайн: 24.09 23:59__   \n",
    "__Жесткий дедлайн: 27.09 23:59__\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит реализовать систему, предлагающую удачное продолжение слова или нескольких следующих слов в режиме реального времени по типу тех, которые используются в почте или поисковой строке. За дополнительные баллы полученную систему нужно будет обернуть в пользовательский интерфейс с помощью библиотеки [reflex](https://github.com/reflex-dev/reflex) или аналогов. В этой домашке вам не придется обучать никаких моделей, мы ограничимся n-граммной генерацией.\n",
    "\n",
    "### Структура\n",
    "\n",
    "Это домашнее задание состоит из двух частей: основной и бонусной. В первой вам нужно будет выполнить 5 заданий, по итогам которых вы получите минимально рабочее решение. А во второй, пользуясь тем, что вы уже сделали реализовать полноценную систему подсказки текста с пользовательским интерфейсом. Во второй части мы никак не будем ограничивать вашу фантазию. Делайте что угодно, лишь бы в результате получился удобный фреймворк. Чем лучше у вас будет результат, тем больше баллов вы получите. Если будет совсем хорошо, то мы добавим бонусов сверху по своему усмотрению.\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — 15 баллов. Сдавать задание после жесткого дедлайна нельзя. При сдачи решения после мягкого дедлайна за каждый день просрочки снимается по __одному__ баллу.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Весь код должен быть написан самостоятельно. Чужим кодом для пользоваться запрещается даже с указанием ссылки на источник. В разумных рамках, конечно. Взять пару очевидных строчек кода для реализации какого-то небольшого функционала можно.\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код.\n",
    "\n",
    "При сдаче зададания в anytask вам будет необходимо сдать весь код, а если вы возьметесь за бонусную часть, то еще отчет и видео с демонстрацией вашего UI. За основную часть можно получить до __10-ти__ баллов, а за бонусную – до __5-ти__ баллов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные\n",
    "\n",
    "Для получения текстовых статистик используйте датасет `emails.csv`. Вы можете найти его по [ссылке](https://disk.yandex.ru/d/ikyUhWPlvfXxCg). Он содержит более 500 тысяч электронных писем на английском языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517401"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emails = pd.read_csv('emails.csv')\n",
    "len(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметьте, что данные очень грязные. В каждом письме содержится различная мета-информация, которая будет только мешать при предсказании продолжения текста.\n",
    "\n",
    "__Задание 1 (2 балла).__ Очистите корпус текстов по вашему усмотрению и объясните свой выбор. В идеале обработанные тексты должны содержать только текст самого письма и ничего лишнего по типу ссылок, адресатов и прочих символов, которыми мы точно не хотим продолжать текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_message(message):\n",
    "    message = ' '.join(message.split('\\n\\n')[1:])\n",
    "    norm_message = ''\n",
    "    for char in message:\n",
    "        if char.isalpha():\n",
    "            norm_message += char.lower()\n",
    "        elif char in [' ', '\\n', '\\t'] and len(norm_message) > 0 and norm_message[-1] != ' ':\n",
    "            norm_message += ' '\n",
    "    return norm_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails['message_norm'] = emails['message'].map(lambda x: normalize_message(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails[['message_norm']].to_csv('corpus.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для следующего задания вам нужно будет токенизировать текст. Для этого просто разбейте его по словам. Очевидно, итоговый результат для финального пользователя будет лучше, если ваша система также будет предлагать уместную пунктуацию. Но если вы заметите, что из-за этого падает качество самого текса, то можете удалить все небуквенные символы на этапе токенизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общая схема решения\n",
    "\n",
    "Мы хотим сделать систему, которая будет ускорять набор текста, советуя подходящие продолжения. Для подсказки следующего слова мы будем использовать n-граммную модель. Так как n-граммная модель работает с целыми словами, а советы мы хотим давать в риал-тайме даже когда слово еще не дописано, сперва надо научиться дополнять слово до целого."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнение слова\n",
    "\n",
    "В этой части вам предстоит реализовать метод дополнения слова до целого по его началу (префиксу). Для этого сперва необходимо научиться находить все слова, имеющие определенный префикс. Мы будем вызывать функцию поиска подходящих слов после каждой напечатанной пользователем буквы. Поэтому нам очень важно, чтобы поиск работал как можно быстрее. Простой перебор всех слов занимает $O(|V| \\cdot n)$ времени, где $|V|$ – размер словаря, а $n$ – длина префикса. Мы же напишем [префиксное дерево](https://ru.wikipedia.org/wiki/Префиксное_дерево), которое позволяет искать слова не больше чем за $O(n + mk)$, где $m$ - число подходящих слов, а $k$ – длина суффикса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2 (2 балла).__ Допишите префиксное дерево для поиска слов по префиксу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class PrefixTreeNode:\n",
    "    def __init__(self):\n",
    "        self.children: dict[str, PrefixTreeNode] = {}\n",
    "        self.is_end_of_word = False\n",
    "\n",
    "class PrefixTree:\n",
    "    def __init__(self, vocabulary):\n",
    "        \"\"\"\n",
    "        vocabulary: список всех уникальных токенов в корпусе\n",
    "        \"\"\"\n",
    "        self.root = PrefixTreeNode()\n",
    "        \n",
    "        for word in vocabulary:\n",
    "            self.insert(word)\n",
    "            \n",
    "    def insert(self, word):\n",
    "        node = self.root\n",
    "        for char in word:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = PrefixTreeNode()\n",
    "            node = node.children[char]\n",
    "        node.is_end_of_word = True\n",
    "\n",
    "    def search_prefix(self, prefix):\n",
    "        \"\"\"\n",
    "        Возвращает все слова, начинающиеся на prefix\n",
    "        prefix: str – префикс слова\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        for char in prefix:\n",
    "            if char not in node.children:\n",
    "                return []\n",
    "            node = node.children[char]\n",
    "\n",
    "        result = []\n",
    "        def dfs(curr, path):\n",
    "            if curr.is_end_of_word:\n",
    "                result.append(path)\n",
    "            for ch, child in curr.children.items():\n",
    "                dfs(child, path + ch)\n",
    "\n",
    "        dfs(node, prefix)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = ['aa', 'aaa', 'abb', 'bba', 'bbb', 'bcd']\n",
    "prefix_tree = PrefixTree(vocabulary)\n",
    "\n",
    "assert set(prefix_tree.search_prefix('a')) == set(['aa', 'aaa', 'abb'])\n",
    "assert set(prefix_tree.search_prefix('bb')) == set(['bba', 'bbb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда у нас есть способ быстро находить все слова с определенным префиксом, нам нужно их упорядочить по вероятности, чтобы выбирать лучшее. Будем оценивать вероятность слова по частоте его __встречаемости в корпусе__.\n",
    "\n",
    "__Задание 3 (2 балла).__ Допишите класс `WordCompletor`, который формирует словарь и префиксное дерево, а так же умеет находить все возможные продолжения слова вместе с их вероятностями. В этом классе вы можете при необходимости дополнительно отфильтровать слова, например, удалив все самые редкие. Постарайтесь максимально оптимизировать ваш код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCompletor:\n",
    "    def __init__(self, corpus):\n",
    "        \"\"\"\n",
    "        corpus: list – корпус текстов\n",
    "        \"\"\"\n",
    "        self.freq = {}\n",
    "        den = 0\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                if word not in self.freq:\n",
    "                    self.freq[word] = 0\n",
    "                self.freq[word] += 1\n",
    "                den += 1\n",
    "        for word in self.freq:\n",
    "            self.freq[word] = self.freq[word] / den\n",
    "        self.prefix_tree = PrefixTree(self.freq.keys())\n",
    "\n",
    "    def get_words_and_probs(self, prefix: str):\n",
    "        \"\"\"\n",
    "        Возвращает список слов, начинающихся на prefix,\n",
    "        с их вероятностями (нормировать ничего не нужно)\n",
    "        \"\"\"\n",
    "        words, probs = [], []\n",
    "        possible_words = self.prefix_tree.search_prefix(prefix)\n",
    "        for w in possible_words:\n",
    "            words.append(w)\n",
    "            probs.append(self.freq[w])\n",
    "        return words, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_corpus = [\n",
    "    [\"aa\", \"ab\"],\n",
    "    [\"aaa\", \"abab\"],\n",
    "    [\"abb\", \"aa\", \"ab\", \"bba\", \"bbb\", \"bcd\"],\n",
    "]\n",
    "\n",
    "word_completor = WordCompletor(dummy_corpus)\n",
    "words, probs = word_completor.get_words_and_probs('a')\n",
    "words_probs = list(zip(words, probs))\n",
    "assert set(words_probs) == {('aa', 0.2), ('ab', 0.2), ('aaa', 0.1), ('abab', 0.1), ('abb', 0.1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание следующих слов\n",
    "\n",
    "Теперь, когда мы умеем дописывать слово за пользователем, мы можем пойти дальше и предожить ему следующее слово (или несколько) с учетом дописанного. Для этого мы воспользуемся n-граммной моделью.\n",
    "\n",
    "Напомним, что вероятность последовательности для такой модели записывается по формуле\n",
    "$$\n",
    "P(w_1, \\dots, w_T) = \\prod_{i=1}^T P(w_i \\mid w_{i-1}, \\dots, w_{i-n}).\n",
    "$$\n",
    "\n",
    "$P(w_i \\mid w_{i-1}, \\dots, w_{i-n})$ оценивается по частоте встречаемости n-граммы.   \n",
    "\n",
    "__Задание 4 (2 балла).__ Напишите класс для n-граммной модели. Никакого сглаживания добавлять не надо, мы же не хотим, чтобы модель советовала случайные слова (хоть и очень редко)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:\n",
    "    def __init__(self, corpus, n):\n",
    "        self.n = n\n",
    "        self.freq = {}\n",
    "        den = {}\n",
    "        for sentence in corpus:\n",
    "            for i in range(max(len(sentence) - n, 0)):\n",
    "                if tuple(sentence[i:i + n]) not in self.freq:\n",
    "                    self.freq[tuple(sentence[i:i + n])] = {}\n",
    "                if sentence[i + n] not in self.freq[tuple(sentence[i:i + n])]:\n",
    "                    self.freq[tuple(sentence[i:i + n])][sentence[i + n]] = 0\n",
    "                self.freq[tuple(sentence[i:i + n])][sentence[i + n]] += 1\n",
    "                \n",
    "                if tuple(sentence[i:i + n]) not in den:\n",
    "                    den[tuple(sentence[i:i + n])] = 0\n",
    "                den[tuple(sentence[i:i + n])] += 1\n",
    "                \n",
    "        for sent in self.freq:\n",
    "            for word in self.freq[sent]:\n",
    "                self.freq[sent][word] = self.freq[sent][word] / den[sent]\n",
    "\n",
    "    def get_next_words_and_probs(self, prefix: list):\n",
    "        \"\"\"\n",
    "        Возвращает список слов, которые могут идти после prefix,\n",
    "        а так же список вероятностей этих слов\n",
    "        \"\"\"\n",
    "\n",
    "        next_words, probs = [], []\n",
    "        prefix = prefix[-self.n:]\n",
    "        \n",
    "        if tuple(prefix) not in self.freq:\n",
    "            return next_words, probs\n",
    "        \n",
    "        for word in self.freq[tuple(prefix)]:\n",
    "            next_words.append(word)\n",
    "            probs.append(self.freq[tuple(prefix)][word])\n",
    "\n",
    "        return next_words, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_corpus = [\n",
    "    ['aa', 'aa', 'aa', 'aa', 'ab'],\n",
    "    ['aaa', 'abab'],\n",
    "    ['abb', 'aa', 'ab', 'bba', 'bbb', 'bcd']\n",
    "]\n",
    "\n",
    "n_gram_model = NGramLanguageModel(corpus=dummy_corpus, n=2)\n",
    "\n",
    "next_words, probs = n_gram_model.get_next_words_and_probs(['aa', 'aa'])\n",
    "words_probs = list(zip(next_words, probs))\n",
    "\n",
    "assert set(words_probs) == {('aa', 2/3), ('ab', 1/3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, мы теперь можем объединить два метода в автоматический дописыватель текстов: первый будет дополнять слово, а второй – предлагать продолжения. Хочется, чтобы предлагался список возможных продолжений, из который пользователь сможет выбрать наиболее подходящее. Самое сложное тут – аккуратно выбирать, что показывать, а что нет.   \n",
    "\n",
    "__Задание 5 (2 балла).__ В качестве первого подхода к снаряду реализуйте метод, возвращающий всегда самое вероятное продолжение жадным способом. После этого можно добавить опцию генерации нескольких вариантов продолжений, что сделает метод гораздо лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TextSuggestion:\n",
    "    def __init__(self, word_completor, n_gram_model):\n",
    "        self.word_completor = word_completor\n",
    "        self.n_gram_model = n_gram_model\n",
    "\n",
    "    def suggest_text(self, text, n_words=3, n_texts=1):\n",
    "        \"\"\"\n",
    "        Возвращает возможные варианты продолжения текста (по умолчанию только один)\n",
    "        \n",
    "        text: строка или список слов – написанный пользователем текст\n",
    "        n_words: число слов, которые дописывает n-граммная модель\n",
    "        n_texts: число возвращаемых продолжений (пока что только одно)\n",
    "        \n",
    "        return: list[list[srt]] – список из n_texts списков слов, по 1 + n_words слов в каждом\n",
    "        Первое слово – это то, которое WordCompletor дополнил до целого.\n",
    "        \"\"\"\n",
    "        if len(text) == 0:\n",
    "            return [], []\n",
    "        \n",
    "        if isinstance(text, str) and text[-1] == ' ':\n",
    "            text = text.split()\n",
    "            \n",
    "        def normalize_message(message):\n",
    "            norm_message = ''\n",
    "            for char in message:\n",
    "                if char.isalpha():\n",
    "                    norm_message += char.lower()\n",
    "                elif char in [' ', '\\n', '\\t'] and len(norm_message) > 0 and norm_message[-1] != ' ':\n",
    "                    norm_message += ' '\n",
    "            return norm_message\n",
    "        \n",
    "        if isinstance(text, str):\n",
    "            text = normalize_message(text)\n",
    "        else:\n",
    "            text = normalize_message(' '.join(text)).split()\n",
    "\n",
    "        if isinstance(text, str):\n",
    "            words, probs = self.word_completor.get_words_and_probs(text.split()[-1])\n",
    "            if len(words) == 0:\n",
    "                return [], [text]\n",
    "            else:\n",
    "                text = text.split()[:-1]\n",
    "        else:\n",
    "            words, probs = self.n_gram_model.get_next_words_and_probs(text)\n",
    "            if len(words) == 0:\n",
    "                return [], [' '.join(text)]\n",
    "        \n",
    "        best_words = [words[i] for i in np.argsort(probs)[::-1][:n_texts]]\n",
    "        suggestions = []\n",
    "        texts = []\n",
    "        for word in best_words:\n",
    "            suggestions_word = [word]\n",
    "            text_word = text + [word]\n",
    "            for _ in range(n_words - 1):\n",
    "                words, probs = self.n_gram_model.get_next_words_and_probs(text_word)\n",
    "                if len(words) == 0:\n",
    "                    break\n",
    "                next_word = words[np.argmax(probs)]\n",
    "                suggestions_word.append(next_word)\n",
    "                text_word.append(next_word)\n",
    "            suggestions.append(suggestions_word)\n",
    "            texts.append(text_word)\n",
    "        \n",
    "        return [[text[-1]] + suggestions[0]] if len(suggestions) < n_words + 1 else suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dummy_corpus = [\n",
    "    ['aa', 'aa', 'aa', 'aa', 'ab'],\n",
    "    ['aaa', 'abab'],\n",
    "    ['abb', 'aa', 'ab', 'bba', 'bbb', 'bcd']\n",
    "]\n",
    "\n",
    "word_completor = WordCompletor(dummy_corpus)\n",
    "n_gram_model = NGramLanguageModel(corpus=dummy_corpus, n=2)\n",
    "text_suggestion = TextSuggestion(word_completor, n_gram_model)\n",
    "\n",
    "assert text_suggestion.suggest_text(['aa', 'aa'], n_words=3, n_texts=1) == [['aa', 'aa', 'aa', 'aa']]\n",
    "assert text_suggestion.suggest_text(['abb', 'aa', 'ab'], n_words=2, n_texts=1) == [['ab', 'bba', 'bbb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_suggestion = TextSuggestion(word_completor, n_gram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусная часть: Добавляем UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускать ячейки в юпитере – это хорошо, но будет лучше, если вашим решением действительно можно будет пользоваться. Для этого вам предлагается добавить полноценных User Interface. Мы рекомендуем использовать для этого [reflex](https://github.com/reflex-dev/reflex). Это Python библиотека для создания web-интерфейсом с очень богатым функционалом.\n",
    "\n",
    "Ваша задача – сделать поле для текстового ввода, при наборе текста в котором будут появляться подсказки в реальном времени. Продумайте, как пользователь будет выбирать подсказки, сколько продолжений рекомендавать и так далее. В общем, должно получиться красиво и удобно. В этой части вы можете модифицировать все классы по своему усмотрению и добавлять любые эвристики. Если нужно, то дополнительно обрабатывать текст и вообще делать все, что считаете нужным. \n",
    "\n",
    "За это задание можно получить до __5-ти бонусных баллов__ в зависимости о того, насколько хорошо и удобно у вас получилось. При сдаче задания прикрепите небольшой __отчет__ (полстраницы) с описанием вашей системы, а также __видео__ (1-2 минуты) с демонстрацией работы интерфейса.\n",
    "\n",
    "Мы настоятельно рекомендуем вам оформить код в проект, а не писать в ноутбуке. Но если вам очень хочется писать тут, то хотя бы не меняйте код в предыдущих заданиях, чтобы его можно было нормально оценивать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "notebookId": "53997d2d-afb8-4477-8874-b6d46299f06c",
  "notebookPath": "seminar.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
